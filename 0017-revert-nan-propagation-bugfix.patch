From e93f463bd36f86cef495d8990e12a42010a8d09a Mon Sep 17 00:00:00 2001
From: Diego Herrera <dherrera@redhat.com>
Date: Mon, 30 Dec 2024 19:44:41 -0300
Subject: [PATCH 17/20] revert-nan-propagation-bugfix

---
 .../providers/cpu/math/element_wise_ops.cc    | 28 +++++++++----------
 .../cpu/math/element_wise_ops_test.cc         | 24 ++++++++--------
 2 files changed, 26 insertions(+), 26 deletions(-)

diff --git a/onnxruntime/core/providers/cpu/math/element_wise_ops.cc b/onnxruntime/core/providers/cpu/math/element_wise_ops.cc
index a78ff69e5c..0f40e140b1 100644
--- a/onnxruntime/core/providers/cpu/math/element_wise_ops.cc
+++ b/onnxruntime/core/providers/cpu/math/element_wise_ops.cc
@@ -705,7 +705,7 @@ Status Min_6<float>::Compute(OpKernelContext* ctx) const {
   for (int index = 1; index < inputCount; index++) {
     auto& data_n = *ctx->Input<Tensor>(index);
     ORT_ENFORCE(data_n.Shape() == shape, "All inputs must have the same shape");
-    min = min.array().template min<Eigen::PropagateNaN>(EigenMap<float>(data_n).array());
+    min = min.array().min(EigenMap<float>(data_n).array());
   }
 
   return Status::OK();
@@ -721,15 +721,15 @@ struct Min_8::ComputeImpl {
     ProcessBroadcastSpanFuncs funcs{
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput1<T>().array().template min<Eigen::PropagateNaN>(per_iter_bh.ScalarInput0<T>());
+              per_iter_bh.EigenInput1<T>().array().min(per_iter_bh.ScalarInput0<T>());
         },
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput0<T>().array().template min<Eigen::PropagateNaN>(per_iter_bh.ScalarInput1<T>());
+              per_iter_bh.EigenInput0<T>().array().min(per_iter_bh.ScalarInput1<T>());
         },
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput0<T>().array().template min<Eigen::PropagateNaN>(
+              per_iter_bh.EigenInput0<T>().array().min(
                   per_iter_bh.EigenInput1<T>().array());
         }};
 
@@ -757,10 +757,10 @@ static Status MinMaxMLFloat16(const OpKernel& inst, OpKernelContext* context) {
         EigenVectorArrayMap<Eigen::half> output_vec_map(output, num_elements);
 
         if (is_min) {
-          output_vec_map = input_1_vec_map.template min<Eigen::PropagateNaN>(
+          output_vec_map = input_1_vec_map.min(
               static_cast<Eigen::half>(per_iter_bh.ScalarInput0<MLFloat16>()));
         } else {
-          output_vec_map = input_1_vec_map.template max<Eigen::PropagateNaN>(
+          output_vec_map = input_1_vec_map.max(
               static_cast<Eigen::half>(per_iter_bh.ScalarInput0<MLFloat16>()));
         }
       },
@@ -774,10 +774,10 @@ static Status MinMaxMLFloat16(const OpKernel& inst, OpKernelContext* context) {
         EigenVectorArrayMap<Eigen::half> output_vec_map(output, num_elements);
 
         if (is_min) {
-          output_vec_map = input_0_vec_map.template min<Eigen::PropagateNaN>(
+          output_vec_map = input_0_vec_map.min(
               static_cast<Eigen::half>(per_iter_bh.ScalarInput1<MLFloat16>()));
         } else {
-          output_vec_map = input_0_vec_map.template max<Eigen::PropagateNaN>(
+          output_vec_map = input_0_vec_map.max(
               static_cast<Eigen::half>(per_iter_bh.ScalarInput1<MLFloat16>()));
         }
       },
@@ -794,9 +794,9 @@ static Status MinMaxMLFloat16(const OpKernel& inst, OpKernelContext* context) {
         EigenVectorArrayMap<Eigen::half> output_vec_map(output, num_elements);
 
         if (is_min) {
-          output_vec_map = input_0_vec_map.template min<Eigen::PropagateNaN>(input_1_vec_map);
+          output_vec_map = input_0_vec_map.min(input_1_vec_map);
         } else {
-          output_vec_map = input_0_vec_map.template max<Eigen::PropagateNaN>(input_1_vec_map);
+          output_vec_map = input_0_vec_map.max(input_1_vec_map);
         }
       }};
 
@@ -832,7 +832,7 @@ Status Max_6<float>::Compute(OpKernelContext* ctx) const {
   for (int index = 1; index < inputCount; index++) {
     auto& data_n = *ctx->Input<Tensor>(index);
     ORT_ENFORCE(data_n.Shape() == shape, "All inputs must have the same shape");
-    max = max.array().template max<Eigen::PropagateNaN>(EigenMap<float>(data_n).array());
+    max = max.array().max(EigenMap<float>(data_n).array());
   }
 
   return Status::OK();
@@ -848,15 +848,15 @@ struct Max_8::ComputeImpl {
     ProcessBroadcastSpanFuncs funcs{
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput1<T>().array().template max<Eigen::PropagateNaN>(per_iter_bh.ScalarInput0<T>());
+              per_iter_bh.EigenInput1<T>().array().max(per_iter_bh.ScalarInput0<T>());
         },
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput0<T>().array().template max<Eigen::PropagateNaN>(per_iter_bh.ScalarInput1<T>());
+              per_iter_bh.EigenInput0<T>().array().max(per_iter_bh.ScalarInput1<T>());
         },
         [](BroadcastHelper& per_iter_bh) {
           per_iter_bh.OutputEigen<T>() =
-              per_iter_bh.EigenInput0<T>().array().template max<Eigen::PropagateNaN>(
+              per_iter_bh.EigenInput0<T>().array().max(
                   per_iter_bh.EigenInput1<T>().array());
         }};
 
diff --git a/onnxruntime/test/providers/cpu/math/element_wise_ops_test.cc b/onnxruntime/test/providers/cpu/math/element_wise_ops_test.cc
index b2e9034653..95c928c1c1 100644
--- a/onnxruntime/test/providers/cpu/math/element_wise_ops_test.cc
+++ b/onnxruntime/test/providers/cpu/math/element_wise_ops_test.cc
@@ -1581,7 +1581,7 @@ TEST(MathOpTest, Min_12_Float_Nan) {
   }
 }
 
-TEST(MathOpTest, Min_12_Float_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Min_12_Float_Nan_with_scalar) {
   OpTester test("Min", 12);
   test.AddInput<float>("data_1", {3, 1},
                        {std::numeric_limits<float>::quiet_NaN(), -0.5f, 0.5f});
@@ -1600,7 +1600,7 @@ TEST(MathOpTest, Min_12_Float_Nan_with_scalar) {
   }
 }
 
-TEST(MathOpTest, Min_12_Float_with_scalar_Nan) {
+TEST(MathOpTest, DISABLED_Min_12_Float_with_scalar_Nan) {
   OpTester test("Min", 12);
   test.AddInput<float>("data_1", {2, 2},
                        {0.25f, -0.25f, -0.5f, 0.5f});
@@ -1667,7 +1667,7 @@ TEST(MathOpTest, Min_12_Double_Nan) {
   }
 }
 
-TEST(MathOpTest, Min_12_Double_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Min_12_Double_Nan_with_scalar) {
   OpTester test("Min", 12);
   test.AddInput<double>("data_1", {3, 1},
                         {std::numeric_limits<double>::quiet_NaN(), -0.5, 0.5});
@@ -1686,7 +1686,7 @@ TEST(MathOpTest, Min_12_Double_Nan_with_scalar) {
   }
 }
 
-TEST(MathOpTest, Min_12_Double_with_scalar_Nan) {
+TEST(MathOpTest, DISABLED_Min_12_Double_with_scalar_Nan) {
   OpTester test("Min", 12);
   test.AddInput<double>("data_1", {2, 2},
                         {0.25, -0.25, -0.5, 0.5});
@@ -1875,7 +1875,7 @@ TEST(MathOpTest, Min_13_Float16_VectorMatrix) {
                      0.5f, 0.0f, 1.0f, 1.0f});
 }
 
-TEST(MathOpTest, Min_13_Float16_Nan) {
+TEST(MathOpTest, DISABLED_Min_13_Float16_Nan) {
   TestFloat16MinMax("Min",
                     {4, 1}, {-1.0f, std::numeric_limits<float>::quiet_NaN(), 1.0f, 0.5f},
                     {4, 1}, {0.5f, 1.0f, 0.25f, std::numeric_limits<float>::quiet_NaN()},
@@ -1883,7 +1883,7 @@ TEST(MathOpTest, Min_13_Float16_Nan) {
                     {-1.0f, std::numeric_limits<float>::quiet_NaN(), 0.25f, std::numeric_limits<float>::quiet_NaN()});
 }
 
-TEST(MathOpTest, Min_13_Float16_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Min_13_Float16_Nan_with_scalar) {
   TestFloat16MinMax("Min",
                     {3, 1}, {-1.0f, std::numeric_limits<float>::quiet_NaN(), 1.0f},
                     {1}, {0.25f},
@@ -2015,7 +2015,7 @@ TEST(MathOpTest, Max_12_Float_Nan) {
   }
 }
 
-TEST(MathOpTest, Max_12_Float_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Max_12_Float_Nan_with_scalar) {
   OpTester test("Max", 12);
   test.AddInput<float>("data_1", {3, 1},
                        {std::numeric_limits<float>::quiet_NaN(), -0.5f, 0.5f});
@@ -2034,7 +2034,7 @@ TEST(MathOpTest, Max_12_Float_Nan_with_scalar) {
   }
 }
 
-TEST(MathOpTest, Max_12_Float_with_scalar_Nan) {
+TEST(MathOpTest, DISABLED_Max_12_Float_with_scalar_Nan) {
   OpTester test("Max", 12);
   test.AddInput<float>("data_1", {2, 2},
                        {0.25f, -0.25f, -0.5f, 0.5f});
@@ -2101,7 +2101,7 @@ TEST(MathOpTest, Max_12_Double_Nan) {
   }
 }
 
-TEST(MathOpTest, Max_12_Double_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Max_12_Double_Nan_with_scalar) {
   OpTester test("Max", 12);
   test.AddInput<double>("data_1", {3, 1},
                         {std::numeric_limits<double>::quiet_NaN(), -0.5, 0.5});
@@ -2120,7 +2120,7 @@ TEST(MathOpTest, Max_12_Double_Nan_with_scalar) {
   }
 }
 
-TEST(MathOpTest, Max_12_Double_with_scalar_Nan) {
+TEST(MathOpTest, DISABLED_Max_12_Double_with_scalar_Nan) {
   OpTester test("Max", 12);
   test.AddInput<double>("data_1", {2, 2},
                         {0.25, -0.25, -0.5, 0.5});
@@ -2277,7 +2277,7 @@ TEST(MathOpTest, Max_13_Float16_VectorMatrix) {
                      1.0f, 1.0f, 2.0f});
 }
 
-TEST(MathOpTest, Max_13_Float16_Nan) {
+TEST(MathOpTest, DISABLED_Max_13_Float16_Nan) {
   TestFloat16MinMax("Max",
                     {4, 1}, {-1.0f, std::numeric_limits<float>::quiet_NaN(), 1.0f, 0.5f},
                     {4, 1}, {0.5f, 1.0f, 0.25f, std::numeric_limits<float>::quiet_NaN()},
@@ -2285,7 +2285,7 @@ TEST(MathOpTest, Max_13_Float16_Nan) {
                     {0.5f, std::numeric_limits<float>::quiet_NaN(), 1.0f, std::numeric_limits<float>::quiet_NaN()});
 }
 
-TEST(MathOpTest, Max_13_Float16_Nan_with_scalar) {
+TEST(MathOpTest, DISABLED_Max_13_Float16_Nan_with_scalar) {
   TestFloat16MinMax("Max",
                     {3, 1}, {-1.0f, std::numeric_limits<float>::quiet_NaN(), 1.0f},
                     {1}, {0.25f},
-- 
2.47.1

